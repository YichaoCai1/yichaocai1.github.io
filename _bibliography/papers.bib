@article{cai2023clap,
  abbr = {ECCV 2024},
  author = {Cai, Yichao and Liu, Yuhang and Zhang, Zhen and Shi, Javen Qinfeng},
  title = {CLAP: Isolating Content from Style through Contrastive Learning with Augmented Prompts},
  journal = {arXiv},
	DOI = {10.48550/arXiv.2311.16445},
	year = {2023},
	abstract = {Contrastive vision-language models, such as CLIP, have garnered considerable attention for various dowmsteam tasks, mainly due to the remarkable ability of the learned features for generalization. However, the features they learned often blend content and style information, which somewhat limits their generalization capabilities under distribution shifts. To address this limitation, we adopt a causal generative perspective for multimodal data and propose contrastive learning with data augmentation to disentangle content features from the original representations. To achieve this, we begins with exploring image augmentation techniques and develop a method to seamlessly integrate them into pre-trained CLIP-like models to extract pure content features. Taking a step further, recognizing the inherent semantic richness and logical structure of text data, we explore the use of text augmentation to isolate latent content from style features. This enables CLIP-like model's encoders to concentrate on latent content information, refining the learned representations by pre-trained CLIP-like models. Our extensive experiments across diverse datasets demonstrate significant improvements in zero-shot and few-shot classification tasks, alongside enhanced robustness to various perturbations. These results underscore the effectiveness of our proposed methods in refining vision-language representations and advancing the state-of-the-art in multimodal learning.},
	preprint = {https://arxiv.org/abs/2311.16445},
  osf ={https://github.com/YichaoCai1/CLAP},
  html={https://yichaocai.com/clap_paper.github.io/},
	selected = {true},
  preview={clap_preview.png},
  type = {Journal Article}
}

@inproceedings{cai2023robust,
  abbr = {ICUS 2023},
  title={Robust Real-Time Curb Detection for Autonomous Sanitation Vehicles},
  author={Cai, Yichao and Ou, Kejun and Li, Dachuan and Zhang, Yuanfang and Zhou, Xiao and Mou, Xingang},
  booktitle={2023 IEEE International Conference on Unmanned Systems (ICUS)},
  pages={475--481},
  year={-2023},
  abstract = {Curb detection is a key enabling functionality for the precise curb-following sanitation control of autonomous sanitation vehicles. The robust and efficient curb detection in complex environments is still a challenging issue. In this paper, we propose a novel semantic segmentation-based framework for curb detection using monocular bird's eye-view images. We employ a lightweight segmentation network based on HRNet to extract the drivable area. A zero-shot post-processing approach is proposed to extract a candidate point set from the segmented image for robust curve fitting. In addition, we propose a modified RANSAC fitting approach that accounts for outlier points to achieve dynamic order curve fitting and curb representation. Experimental results in complex sanitation scenarios demonstrate the efficiency, accuracy, and robustness of the proposed approach.},
  preview={curb_preview.png},
  html={https://doi.org/10.1109/ICUS58632.2023.10318292},
  selected = {true},
  organization={IEEE}
}

@inproceedings{cai2019head,
  abbr = {ICMV 2018},
  title={Head pose estimation with neural networks from surveillant images},
  author={Cai, Yichao and Zhou, Xiao and Li, Dachuan and Ming, Yifei and Mou, Xingang},
  booktitle={Eleventh International Conference on Machine Vision (ICMV 2018)},
  volume={11041},
  pages={267--274},
  year={-2019},
  abstract={Estimating head pose of pedestrians is a crucial task in autonomous driving system. It plays a significant role in many research fields, such as pedestrian intention judgment and human-vehicle interaction, etc. While most of the current studies focus on driver’s-view images, we reckon that surveillant images are also worthy of attention since more global information can be obtained from them than driver’s-view images. In this paper, we propose a method for head pose estimation from surveillant images. This approach consists of two stages, head detection and pose estimation. Since the head of pedestrian takes up a very small number of pixels in a surveillant image, a two-step strategy is used to improve the performance in head detection. Firstly, we train a model to extract body region from the source image. Secondly, a head detector is trained to locate head position from the extracted body regions. We use YOLOv3 as our detection network for both body and head detection. For head pose estimation, we treat it as classification task of 10 categories. We use ResNet-50 as the backbone of the classifier, of which the input is the result of head detection. A serial of experiments demonstrate the good performance of our proposed method.},
  preview={headpose_preview.png},
  html={https://doi.org/10.1117/12.2523090},
  organization={SPIE}
}

@article{cai2018robust,
  abbr = {Sensors},
  title={Robust drivable road region detection for fixed-route autonomous vehicles using map-fusion images},
  author={Cai, Yichao and Li, Dachuan and Zhou, Xiao and Mou, Xingang},
  journal={Sensors},
  volume={18},
  number={12},
  pages={4158},
  year={2018},
  abstract = {Environment perception is one of the major issues in autonomous driving systems. In particular, effective and robust drivable road region detection still remains a challenge to be addressed for autonomous vehicles in multi-lane roads, intersections and unstructured road environments. In this paper, a computer vision and neural networks-based drivable road region detection approach is proposed for fixed-route autonomous vehicles (e.g., shuttles, buses and other vehicles operating on fixed routes), using a vehicle-mounted camera, route map and real-time vehicle location. The key idea of the proposed approach is to fuse an image with its corresponding local route map to obtain the map-fusion image (MFI) where the information of the image and route map act as complementary to each other. The information of the image can be utilized in road regions with rich features, while local route map acts as critical heuristics that enable robust drivable road region detection in areas without clear lane marking or borders. A neural network model constructed upon the Convolutional Neural Networks (CNNs), namely FCN-VGG16, is utilized to extract the drivable road region from the fused MFI. The proposed approach is validated using real-world driving scenario videos captured by an industrial camera mounted on a testing vehicle. Experiments demonstrate that the proposed approach outperforms the conventional approach which uses non-fused images in terms of detection accuracy and robustness, and it achieves desirable robustness against undesirable illumination conditions and pavement appearance, as well as projection and map-fusion errors.},
  selected = {true},
  html={https://www.mdpi.com/1424-8220/18/12/4158},
  preview={autonomousdriving_preview.png},
  publisher={MDPI}
}

@inproceedings{mou2018state,
  abbr = {ICIVC 2018},
  title={State Recognition of Electric Control Cabinet Switches Based on CNNs},
  author={Mou, Xingang and Leng, Conglin and Zhou, Xiao and Cai, Yichao},
  booktitle={2018 IEEE 3rd International Conference on Image, Vision and Computing (ICIVC)},
  pages={183--187},
  year={-2018},
  abstract = {When using the CNNs for image processing, it is more easy to learn the invariant features of the image without too much preprocessing. This is important for the task of image classification and recognition which need to extract deep features of the object. This paper proposes a switch state recognition algorithm which combines digital image processing technique and CNNs for the electrical control cabinet switches in substations. Firstly, use perspective transformation method to rectify the original image which is positioned and segmented using the projections of horizontal and vertical gradient. Then label the segmented images to get sample sets for training CNNs later. After that, design the architecture of CNNs with image processing knowledge and use the designed CNNs to extract the key features of switch state, build and train the CNNs model using labeled "disconnected" and "closed" switches. Finally, the state of the switch is recognized by the trained model and showed in the image. The experimental results show that the algorithm has the characteristics of high recognition accuracy, good robustness and fast recognition speed.},
  html={https://doi.org/10.1109/ICIVC.2018.8492853},
  preview={switchstate_preview.png},
  organization={IEEE}
}

@article{RN7179,
   author = {Bulla, Martin and Valcu, Mihai and Kempenaers, Bart},
   title = {格式模板Still no evidence for disruption of global patterns of nest predation in shorebirds},
   journal = {Science},
   DOI = {https://doi.org/10.1101/2021.02.17.431576},
   year = {-2021},
   abstract = {Many shorebird species are rapidly declining (Piersma et al. 2016; Munro 2017; Studds et al. 2017), but it is not always clear why. Deteriorating and disappearing habitat, e.g. due to intensive agriculture (Donal et al. 2001; Kentie et al. 2013; Kentie et al. 2018), river regulation (Nebel et al. 2008) or mudflat reclamation (Ma et al. 2014; Larson 2017), and hunting (Reed et al. 2018; Gallo-Cajiao et al. 2020) are some of the documented causes. A recent study suggests yet another possible cause of shorebird decline: a global increase in nest predation (Kubelka et al. 2018). The authors compiled an impressive dataset on patterns of nest predation in shorebirds and their analyses suggest that global patterns of nest predation have been disrupted by climate change, particularly in the Arctic. They go as far as to conclude that the Arctic might have become an ecological trap (Kubelka et al. 2018). Because these findings might have far-reaching consequences for conservation and related political decisions, we scrutinized the study and concluded that the main conclusions of Kubelka et al. (2018) are invalid (Bulla et al. 2019a). The authors then responded by reaffirming their conclusions (Kubelka et al. 2019b).
   
   Here, we evaluate some of Kubelka et al.’s (2019b) responses, including their recent erratum (2020), and show that the main concerns about the original study still hold. Specifically, (1) we reaffirm that Kubelka et al.’s (2018) original findings are confounded by study site. Hence, their conclusions are over-confident because of pseudo-replication. (2) We reiterate that there is no statistical support for the assertion that predation rate has changed in a different way in the Arctic compared to other regions. The relevant test is an interaction between a measure of time (year or period) and a measure of geography (e.g., Arctic vs the rest of the world). The effect of such an interaction is weak, uncertain and statistically non-significant, which undermines Kubelka et al.’s (2018) key conclusion. (3) We further confirm that the suggested general increase in predation rates over time is at best a weak and uncertain trend. The most parsimonious hypothesis for the described results is that the temporal changes in predation rate are an artefact of temporal changes in methodology and data quality. Using only high-quality data, i.e. directly calculated predation rates, reveals no overall temporal trend in predation rate. Below we elaborate in detail on each of these points.
   
   We conclude that (i) there is no evidence whatsoever that the pattern in the Arctic is different from that in the rest of the world and (ii) there is no solid evidence for an increase in predation rate over time. While we commend Kubelka et al. for compiling and exploring the data, we posit that the data underlying their study, and perhaps all currently available data, are not sufficient (or of sufficient quality) to test their main hypotheses. We call for standardized and consistent data collection protocols and experimental validation of current methods for estimating nesting success.},
   bibtex_show={true},
   html = {https://www.science.org/do/10.1126/comment.757997/full/},
   pdf= {https://doi.org/10.1101/2021.02.17.431576},
   osf ={https://github.com/MartinBulla/Still_no_evidence/},
   preview={Bulla_2021.jpg},
   type = {Journal Article}
}

